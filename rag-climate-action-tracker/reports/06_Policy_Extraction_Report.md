# Policy_Extraction_Report.md

## `climate_tracker/scripts/policy_extraction.py`


This script is the second stage of a policy analysis pipeline designed to extract structured, interpretable answers from retrieved climate policy evidence. It takes as input the country-level Markdown files generated by `information_retrieval.py` and produces a json file `output/policies_targets_output.json` and produces per-country Markdown summaries in `policy_targets_pages/`. These outputs answer five predefined policy questions with structured fields.

---

**Objective**

This script addresses Sylvan's directive to *"improve the relevance of the results"* by:

* Breaking down complex policy questions into component parts
* Using enhanced NLP techniques to extract structured, explainable answers
* Outputting:

  * `yes_no` answers
  * standardised explanations
  * the original quote
  * confidence scores
  * and source metadata

---

**Core NLP Pipeline & Design Rationale**

We designed a layered pipeline to move from noisy text to structured understanding:

**Step 1: Enhanced NER Tagging**

* Uses regex + SpaCy NER to detect:

  * Years (`YEAR`) via explicit years, phrases like "by 2060", or even "mid-century"
  * Sectors (`SECTOR`) via keywords: electricity, agriculture, etc.
  * Policy types (`POLICY_TYPE`) and targets (`TARGET`) based on presence of legal terms or phrases like "net zero"

Used Regex because...
* It is used to find specific textual patterns in messy, natural language — especially when named entity recognition (NER) may not catch everything.
* It is precise and works well for consistent formats (e.g., dates), and complements SpaCy, which might miss less common phrasings or fail to extract context like "mid-century". 

Used SpaCy because...
* It provides language understanding beyond string-matching
* Dependency parsing is critical for capturing who is doing what and for detecting negation
* It helps make rules more robust to sentence variation

Together using Regex and Spacy allows you to use exact formats (e.g. "2030") - Regex, semantics (e.g. negation) - SpaCy, policy-specific terms - both, mutli-word expressions - SpaCy, fuzzy patterns or edge cases - Regex


**Step 2: Dependency Parsing**

* Extracts negations and underlying grammatical relationships (e.g., subject-verb-object structure)
* Example: Detects negated targets via dependency labels (e.g., `neg`), but does not override the answer. 


**Step 3: Semantic Scoring (Cosine Similarity)**

* While `climate_tracker/climate_trackr/scripts/information_retrieval.py` initially ranks and selects the top chunk based on similarity, the score is recomputed in `climate_tracker/climate_tracker/scripts/policy_extraction.py` from scratch using the same embedding model (BAAI/bge-m3). 
  * This ensures that answers with low semantic relevance to the original question are filtered out early in the extraction pipeline. 
  * While the retrieval stage (in script information_retrieval.py) already selects top-ranked chunks, recalculating similarity in policy_extraction.py, helps guard against off-topic or overly generic responses that may have slipped through initial retrieval. 

* That being said:
  * The cosine similarity in policy_extraction.py is recomputed between the question and this aggregated answer, rather than the individual most relevant chunk. This may dilute the signal, especially if the answer contains filler text or less relevant evidence.
  * Since the output from running information_retrieval.py, often includes subjective/evaluative text from CAT (e.g. “We rate this as acceptable”), this may increase the similarity score without directly answering the question. 

* If the cosine similarity is below a strict threshold of 0.70, the chunk is labeled as low_confidence, signaling that the evidence may be off-topic or too vague to support a definitive answer. 


**Step 4: Rule-Based Extraction**

Implements block-specific rules:

* For electricity net zero:
  - Returns "yes" if text contains both "net-zero" and any electricity-related term (electricity, power, grid, renewable energy)
  - Returns "soft_yes" if the evidence is scenario-based or if terms appear in different contexts
  - Checks the entire text block rather than requiring terms to be in the same passage

* For energy efficiency:
  - Uses a three-tier classification system:
    1. "yes" if both policy (law/strategy) and specific target exist
    2. "soft_yes" if policy exists but target is unclear
    3. "soft_yes" if only efficiency themes are mentioned without clear policy
  - Checks for efficiency-related terms (efficiency, energy, saving, reduce)
  - Looks for target indicators (target, goal, objective, percentage reductions)


---

**Handling Negation Intelligently**
Initially, any negation (`neg` dependency) would immediately assign `yes_no = no`. This led to false negatives (e.g., “not yet implemented” or “needs more support”).

**Improvement:** Negation is now detected and preserved, but:

* It does not override a confident `yes` or `soft_yes`
* Instead, we append a soft warning to the explanation:

  * `"(Note: negation present in sentence, which may weaken the claim.)"`

---


**Output Strategy**

* Each country gets a Markdown file. 
* These summaries enable human analysts or auditors to rapidly review decisions per country.

* E.g. the first section of  `argentina_policy_targets.md`, looks like this: 

```markdown

Net Zero Target

- **Answer**: `yes`

- **Explanation**: Mentions net zero target.

- **Year(s)**: 2022, 2050

- **Confidence**: 0.8189

- **Source URL**: https://climateactiontracker.org/countries/argentina/targets/

> Answer/Evidence (Similarity: 0.8251): Further information on how the CAT rates countries (against modelled domestic pathways and fair share) can be found here. **    ## Net zero and other long-term target(s)   We evaluate the net zero target as: Poor. **   In November 2022, Argentina submitted a lon
```

* We can compare the above with the output from running the pure vector search from `climate_tracker/scripts/05_information_retrieval.py`: 
```markdown

Question 1: Does the country have a net zero target, and if so, what year is the target set for?

**Answer/Evidence (Similarity: 0.8251):**
> **Further information on how the CAT rates countries (against modelled domestic pathways and fair share) can be found** **here****. **    ## Net zero and other long-term target(s)   We evaluate the net zero target as: **Poor. **   In November 2022, Argentina submitted a long-term strategy (LTS) to the UNFCCC that includes a target to reach GHG neutrality by 2050 (Government of Argentina, 2022a).

**Source URL:** [https://climateactiontracker.org/countries/argentina/targets/](https://climateactiontracker.org/countries/argentina/targets/)
```

(Note: we can see that the year section does pick up the year of 2022 from the source, as well as the required 2050). 


---

**Why this Order? (NER → Parse → Embed → Rules)**

* NER first filters sentences and assigns semantic tags
* Dependency parsing checks structure and negations
* Cosine similarity confirms if the evidence is semantically close to the question
* Rules then determine final values

> This combination balances generalisability (via embeddings) with control and explainability (via rules).

---
**Conclusion**
This script exemplifies a hybrid NLP strategy tailored for climate policy extraction. It combines retrieval validation (via embeddings) with symbolic reasoning (NER, rules, negation) to achieve trustworthy, explainable, and relevant outputs.

---
**Evaluation**

Next time, compute cossine similarity per tag, based on the proximity to specific keywords (e.g., "by 2050"). E.g:
- If the text contains "net zero by 2050" → confidence for net_zero_target should be boosted.
- If "carbon tax" appears near "2023" → higher confidence for carbon_pricing.
