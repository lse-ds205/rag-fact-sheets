{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Demo of the Pillar Assessment Tool\n",
    "\n",
    "### 0. Setup\n",
    "\n",
    "First import neccissary modules from `scripts\\climate_policy_pipelines\\cp1`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added c:\\Users\\User\\GitHub\\group-6-final-project to sys.path\n"
     ]
    }
   ],
   "source": [
    "# Import necessary modules\n",
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Get the absolute path of the project root directory\n",
    "notebook_dir = Path(os.getcwd())  \n",
    "project_root = notebook_dir.parent.parent  # Go up TWO levels instead of one\n",
    "\n",
    "# Add project root to Python path\n",
    "if str(project_root) not in sys.path:\n",
    "    sys.path.insert(0, str(project_root))\n",
    "    print(f\"Added {project_root} to sys.path\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scripts.climate_policy_pipelines.cp1.pipeline import run_cp1a_assessment\n",
    "from scripts.climate_policy_pipelines.cp1.pipeline import run_cp1a_assessment_large_context\n",
    "from scripts.climate_policy_pipelines.cp1.pipeline import run_cp1b_assessment\n",
    "from scripts.climate_policy_pipelines.cp1.pipeline import get_chunks_for_cp1a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scripts.climate_policy_pipelines.cp1.prompts import cp1a_criterion_1_prompt\n",
    "from scripts.climate_policy_pipelines.cp1.prompts import cp1a_final_assessment_prompt\n",
    "from scripts.climate_policy_pipelines.cp1.prompts import comprehensive_assessment_prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CP1a Asessment\n",
    "\n",
    "CP1a is defined as: **Does the country have a framework climate law or equivalent?**\n",
    "\n",
    "Based on the ASCOR methodology, a country is assessed as ‘Yes’ if it has a framework climate law that fulfils either 1&2&3 or just 4 of \n",
    "the following criteria:\n",
    "\n",
    "1. It sets a strategic direction for decarbonisation\n",
    "\n",
    "2. It is enshrined in law\n",
    "\n",
    "3. It sets out at least one of the following obligations\n",
    "\n",
    "4. Also check this In exceptional cases, the combination of a broad environmental law and a clearly linked executive climate strategy may be sufficient to meet these criteria\n",
    "\n",
    "Each criteria is handled by a different LLM, which recieves a criteria-specific prompt + retrieved context relevant to that critera.\n",
    "\n",
    "We can see the prompt for criteria 1 here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "System Message Template:\n",
      "You are an expert legal analyst specializing in climate legislation. \n",
      "    Your task is to evaluate whether a climate law sets a strategic direction for decarbonisation.\n",
      "    \n",
      "    A law meets this criterion if it includes a clear statement to meet the goals of the Paris Agreement \n",
      "    OR a national long-term decarbonisation target.\n",
      "    \n",
      "    For any claims you make, you **MUST** include the page number and document citation in the format (page X, doc Y).\n",
      "\n",
      "     \n",
      "    Respond with only 'YES' or 'NO' followed by a brief explanation.\n",
      "\n",
      "==================================================\n",
      "\n",
      "Human Message Template:\n",
      "Context: {context}\n",
      "\n",
      "Does this law set a strategic direction for decarbonisation?\n"
     ]
    }
   ],
   "source": [
    "# Print the system message template\n",
    "print(\"System Message Template:\")\n",
    "print(cp1a_criterion_1_prompt.messages[0].prompt.template)\n",
    "\n",
    "print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "\n",
    "# Print the human message template  \n",
    "print(\"Human Message Template:\")\n",
    "print(cp1a_criterion_1_prompt.messages[1].prompt.template)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to ASCOR methodology, if 1,2,3 are satisfied or 4 is satisfied, then CP1a is answered as Yes.\n",
    "\n",
    "We therefore use another LLM to evalute the overall prompt based on the other LLM's responses. Lets have a look at its prompt:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "System Message Template:\n",
      "You are an expert legal analyst making a final assessment of climate legislation.\n",
      "    \n",
      "    A country is assessed as 'YES' for having framework climate law if:\n",
      "    - Criteria 1, 2, AND 3 are all satisfied, OR\n",
      "    - Criterion 4 is satisfied (exceptional case)\n",
      "   \n",
      "    For any claims you make, you **MUST** include the page number and document citation in the format (page X, doc Y).\n",
      "\n",
      "    Based on the individual assessments, provide a final 'YES' or 'NO' answer with reasoning.\n",
      "\n",
      "==================================================\n",
      "\n",
      "Human Message Template:\n",
      "Individual criterion assessments:\n",
      "    Criterion 1 (Strategic direction): {criterion_1_result}\n",
      "    Criterion 2 (Enshrined in law): {criterion_2_result}\n",
      "    Criterion 3 (Obligations): {criterion_3_result}\n",
      "    Criterion 4 (Exceptional case): {criterion_4_result}\n",
      "    \n",
      "    What is the final assessment?\n"
     ]
    }
   ],
   "source": [
    "# Print the system message template\n",
    "print(\"System Message Template:\")\n",
    "print(cp1a_final_assessment_prompt.messages[0].prompt.template)\n",
    "\n",
    "print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "\n",
    "# Print the human message template  \n",
    "print(\"Human Message Template:\")\n",
    "print(cp1a_final_assessment_prompt.messages[1].prompt.template)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### What content does the assessment use?\n",
    "\n",
    "Lets have a look at the chunks it retrieves in order to inform its assessment of each criteria.\n",
    "\n",
    "If you look at `get_chunks_for_cp1a` in `scripts/climate_policy_pipelines/cp1.pipeline`, you can see that these are 4 sentences it embeds and retrives chunks for, retrieving 100 chunks for each CP1a criteria \n",
    "\n",
    "```\n",
    "cp1a_prompts = [\n",
    "    \"strategic direction for decarbonisation Paris Agreement national long-term target\",\n",
    "    \"climate law enshrined in law legally binding framework\",\n",
    "    \"obligations carbon budgets emissions targets monitoring requirements\",\n",
    "    \"environmental law executive climate strategy broad framework\"\n",
    "]\n",
    "```\n",
    "\n",
    "Lets have a look at just what `cp1a_criteria_1_search_prompt` retrieves:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cp1a_criteria_1_search_prompt = \"strategic direction for decarbonisation Paris Agreement national long-term target\"\n",
    "get_chunks_for_cp1a(cp1a_criteria_1_search_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Demo assessment \n",
    "\n",
    "Lets run an assessment for Albania"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No documents found for country code: POL\n",
      "No documents found for country code: POL\n",
      "No documents found for country code: POL\n",
      "No documents found for country code: POL\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"Based on the individual criterion assessments, the final assessment is:\\n\\nNO\\n\\nReasoning: Since Criteria 1, 2, and 3 are all 'NO', the country does not meet the first condition for having a framework climate law. Additionally, Criterion 4 is also 'NO', which means the exceptional case condition is not satisfied either. Therefore, the country does not have a framework climate law. (page X, doc Y)\", additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 89, 'prompt_tokens': 292, 'total_tokens': 381, 'completion_tokens_details': None, 'prompt_tokens_details': None}, 'model_name': 'meta-llama/Meta-Llama-3.1-70B-Instruct', 'system_fingerprint': None, 'id': 'chatcmpl-519a50bb2b784a2086b662720ce8179b', 'service_tier': None, 'finish_reason': 'stop', 'logprobs': None}, id='run--ee1a111d-aa12-46c3-b157-8105eca1d995-0', usage_metadata={'input_tokens': 292, 'output_tokens': 89, 'total_tokens': 381, 'input_token_details': {}, 'output_token_details': {}})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run_cp1a_assessment(country=\"POL\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CP1a Large Context Window Asessment\n",
    "\n",
    "Another tool just uses one, more powerful large context window LLM to assess all components of CP1a togehter instead of seperatley. Here is the prompt:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "System Message Template:\n",
      "You are an expert legal analyst specializing in climate legislation assessment. \n",
      "\n",
      "Your task is to evaluate whether a country has a framework climate law based on specific criteria and provide a structured markdown assessment.\n",
      "\n",
      "EVALUATION CRITERIA:\n",
      "A country is assessed as 'YES' if it has a framework climate law that fulfils ALL of criteria 1, 2, AND 3, OR criterion 4:\n",
      "\n",
      "1. STRATEGIC DIRECTION: Sets a strategic direction for decarbonisation (must include a clear statement to meet the goals of the Paris Agreement OR a national long-term decarbonisation target)\n",
      "\n",
      "2. ENSHRINED IN LAW: Is enshrined in law (must be legislative rather than executive, except in particular political systems)\n",
      "\n",
      "3. OBLIGATIONS: Sets out at least one of the following obligations:\n",
      "   - Meeting a national target\n",
      "   - Developing, revising, implementing or complying with domestic plans, strategies or policies\n",
      "   - Developing policy instruments such as regulation, taxation or public spending in support of climate goals\n",
      "\n",
      "4. EXCEPTIONAL CASE: The combination of a broad environmental law AND a clearly linked executive climate strategy may be sufficient to meet these criteria\n",
      "\n",
      "ASSESSMENT LOGIC:\n",
      "- If criteria 1, 2, AND 3 are all satisfied → YES\n",
      "- If criterion 4 is satisfied → YES\n",
      "- Otherwise → NO\n",
      "\n",
      "OUTPUT FORMAT:\n",
      "Provide your assessment in the following markdown format:\n",
      "\n",
      "```markdown\n",
      "# Climate Legislation Assessment: CP 1.a Framework Climate Law\n",
      "\n",
      "## Individual Criterion Evaluation\n",
      "\n",
      "### Criterion 1: Strategic Direction for Decarbonisation\n",
      "**Result:** [YES/NO]\n",
      "**Reasoning:** [Brief explanation of whether the law includes clear Paris Agreement goals or long-term decarbonisation targets]\n",
      "\n",
      "### Criterion 2: Enshrined in Law\n",
      "**Result:** [YES/NO]\n",
      "**Reasoning:** [Brief explanation of whether this is legislative rather than executive]\n",
      "\n",
      "### Criterion 3: Sets Out Obligations\n",
      "**Result:** [YES/NO]\n",
      "**Reasoning:** [Brief explanation of which obligations are present, if any]\n",
      "\n",
      "### Criterion 4: Exceptional Case\n",
      "**Result:** [YES/NO]\n",
      "**Reasoning:** [Brief explanation of whether broad environmental law + executive strategy combination exists]\n",
      "\n",
      "## Final Assessment\n",
      "\n",
      "**Overall Result:** [YES/NO]\n",
      "\n",
      "**Logic Applied:** [Explain whether criteria 1+2+3 are satisfied OR criterion 4 is satisfied]\n",
      "\n",
      "**Conclusion:** [Brief summary of why the country does/does not have a framework climate law]\n",
      "```\n",
      "\n",
      "==================================================\n",
      "\n",
      "Human Message Template:\n",
      "Context: {context}\n",
      "\n",
      "Please evaluate whether this country has a framework climate law based on the provided context. For any claims you make, you **MUST** include the page number and document citation in the format (page X, doc Y).\n"
     ]
    }
   ],
   "source": [
    "# Print the system message template\n",
    "print(\"System Message Template:\")\n",
    "print(comprehensive_assessment_prompt.messages[0].prompt.template)\n",
    "\n",
    "print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "\n",
    "# Print the human message template  \n",
    "print(\"Human Message Template:\")\n",
    "print(comprehensive_assessment_prompt.messages[1].prompt.template)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets see how it performs on the same country:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No documents found for country code: POL\n",
      "No documents found for country code: POL\n",
      "No documents found for country code: POL\n",
      "No documents found for country code: POL\n",
      "Large Context Assessment:\n",
      "Based on the provided context, I must inform that there is no information available to assess the country's framework climate law. The context is empty, and I couldn't find any relevant information to evaluate the criteria.\n",
      "\n",
      "However, I will provide a structured markdown assessment as per your request:\n",
      "\n",
      "```markdown\n",
      "# Climate Legislation Assessment: CP 1.a Framework Climate Law\n",
      "\n",
      "## Individual Criterion Evaluation\n",
      "\n",
      "### Criterion 1: Strategic Direction for Decarbonisation\n",
      "**Result:** NO\n",
      "**Reasoning:** No information is available to determine if the law includes clear Paris Agreement goals or long-term decarbonisation targets.\n",
      "\n",
      "### Criterion 2: Enshrined in Law\n",
      "**Result:** NO\n",
      "**Reasoning:** No information is available to determine if this is legislative rather than executive.\n",
      "\n",
      "### Criterion 3: Sets Out Obligations\n",
      "**Result:** NO\n",
      "**Reasoning:** No information is available to determine if any obligations are present.\n",
      "\n",
      "### Criterion 4: Exceptional Case\n",
      "**Result:** NO\n",
      "**Reasoning:** No information is available to determine if a broad environmental law and executive strategy combination exists.\n",
      "\n",
      "## Final Assessment\n",
      "\n",
      "**Overall Result:** NO\n",
      "\n",
      "**Logic Applied:** Since no information is available, criteria 1, 2, and 3 cannot be satisfied, and criterion 4 cannot be evaluated.\n",
      "\n",
      "**Conclusion:** Based on the lack of information, it is impossible to determine if the country has a framework climate law. Further research and documentation are required to make an accurate assessment.\n",
      "```\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"Based on the provided context, I must inform that there is no information available to assess the country's framework climate law. The context is empty, and I couldn't find any relevant information to evaluate the criteria.\\n\\nHowever, I will provide a structured markdown assessment as per your request:\\n\\n```markdown\\n# Climate Legislation Assessment: CP 1.a Framework Climate Law\\n\\n## Individual Criterion Evaluation\\n\\n### Criterion 1: Strategic Direction for Decarbonisation\\n**Result:** NO\\n**Reasoning:** No information is available to determine if the law includes clear Paris Agreement goals or long-term decarbonisation targets.\\n\\n### Criterion 2: Enshrined in Law\\n**Result:** NO\\n**Reasoning:** No information is available to determine if this is legislative rather than executive.\\n\\n### Criterion 3: Sets Out Obligations\\n**Result:** NO\\n**Reasoning:** No information is available to determine if any obligations are present.\\n\\n### Criterion 4: Exceptional Case\\n**Result:** NO\\n**Reasoning:** No information is available to determine if a broad environmental law and executive strategy combination exists.\\n\\n## Final Assessment\\n\\n**Overall Result:** NO\\n\\n**Logic Applied:** Since no information is available, criteria 1, 2, and 3 cannot be satisfied, and criterion 4 cannot be evaluated.\\n\\n**Conclusion:** Based on the lack of information, it is impossible to determine if the country has a framework climate law. Further research and documentation are required to make an accurate assessment.\\n```\", additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 301, 'prompt_tokens': 587, 'total_tokens': 888, 'completion_tokens_details': None, 'prompt_tokens_details': None}, 'model_name': 'meta-llama/Meta-Llama-3.1-70B-Instruct', 'system_fingerprint': None, 'id': 'chatcmpl-019cc433a3ab432ab867faa20bf0f486', 'service_tier': None, 'finish_reason': 'stop', 'logprobs': None}, id='run--b39a367f-05da-4b2e-a271-f067f4969acf-0', usage_metadata={'input_tokens': 587, 'output_tokens': 301, 'total_tokens': 888, 'input_token_details': {}, 'output_token_details': {}})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run_cp1a_assessment_large_context(country=\"POL\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CP1b Assessment\n",
    "\n",
    "We have also built a tool that automatically evalutes CP1b\n",
    "\n",
    "CP1b is defined by ASCOR as: **Does the country’s framework climate law specify key accountability elements?**\n",
    "\n",
    "\n",
    "A country is assessed as ‘Yes’ if its framework climate law contains all three of the following accountability elements: \n",
    "1. Specification of who is accountable to whom for at least one stated obligation (e.g. accountability of executive to parliament, or private parties to executive authorities) \n",
    "2. Specification of how compliance is assessed for at least one stated obligation (e.g. transparency mechanisms in the form of monitoring, reporting and verification, parliamentary oversight, expert assessments, court proceedings) \n",
    "3. Specification of what happens in the case of non-compliance for at least one stated obligation (e.g. parliamentary intervention, judicial orders, financial penalties). \n",
    "\n",
    "Like with CP1a, these are evaluated separately and then evaluted together by a evaluator LLM. There is additional guidance in the methodology on how to asses these criteria wich is included in the prompts (see `scripts/climate_policy_pipeline/cp1/prompts`)\n",
    "\n",
    "Lets see how the tool performs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No documents found for country code: POL\n",
      "No documents found for country code: POL\n",
      "No documents found for country code: POL\n",
      "Detailed Assessment:\n",
      "Based on the individual criterion assessments, the final assessment is:\n",
      "\n",
      "NO\n",
      "\n",
      "Reasoning: Since all three criteria (Criterion 1, Criterion 2, and Criterion 3) are assessed as 'NO' due to a lack of information, the country's framework climate law does not meet the requirements for a 'YES' assessment. Without sufficient information to evaluate the law, it is impossible to determine whether the law specifies the necessary accountability elements.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"Based on the individual criterion assessments, the final assessment is:\\n\\nNO\\n\\nReasoning: Since all three criteria (Criterion 1, Criterion 2, and Criterion 3) are assessed as 'NO' due to a lack of information, the country's framework climate law does not meet the requirements for a 'YES' assessment. Without sufficient information to evaluate the law, it is impossible to determine whether the law specifies the necessary accountability elements.\", additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 89, 'prompt_tokens': 299, 'total_tokens': 388, 'completion_tokens_details': None, 'prompt_tokens_details': None}, 'model_name': 'meta-llama/Meta-Llama-3.1-70B-Instruct', 'system_fingerprint': None, 'id': 'chatcmpl-2448a8b2374044fe9e93faf1b1a1a56c', 'service_tier': None, 'finish_reason': 'stop', 'logprobs': None}, id='run--249e3f80-4d31-4c0b-bec4-f36c1af6a08a-0', usage_metadata={'input_tokens': 299, 'output_tokens': 89, 'total_tokens': 388, 'input_token_details': {}, 'output_token_details': {}})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run_cp1b_assessment(country=\"POL\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to easily automate assessment so do not want to have justification and only output the yes/no answer, you can set `detailed=False` in the function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_cp1b_assessment(country='ALB', detailed=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the assessment will be the same always because model temperature is set to 0."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "group_rag.venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
