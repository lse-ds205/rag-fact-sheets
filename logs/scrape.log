2025-05-26 11:52:19,426 - group4py.src.scrape.workflow - INFO - Starting NDC document scraping workflow
2025-05-26 11:52:19,426 - group4py.src.scrape.workflow - INFO - Step 1: Retrieving existing documents from database...
2025-05-26 11:52:19,426 - group4py.src.scrape.db_operations - INFO - Retrieving existing documents from database
2025-05-26 11:52:19,426 - group4py.src.database - INFO - <DATABASE> Connecting...
2025-05-26 11:52:19,426 - group4py.src.database - INFO - <DATABASE> Getting engine...
2025-05-26 11:52:19,528 - group4py.src.database - INFO - <DATABASE> Engine created successfully
2025-05-26 11:52:20,831 - group4py.src.database - WARNING - <DATABASE> Connected to database successfully
2025-05-26 11:52:20,934 - group4py.src.scrape.db_operations - INFO - Retrieved 206 existing documents from database
2025-05-26 11:52:20,943 - group4py.src.scrape.workflow - INFO - Found 206 existing documents in database
2025-05-26 11:52:20,943 - group4py.src.scrape.workflow - INFO - Step 2: Scraping fresh documents from website...
2025-05-26 11:52:20,944 - WDM - INFO - ====== WebDriver manager ======
2025-05-26 11:52:21,155 - WDM - INFO - Get LATEST chromedriver version for google-chrome
2025-05-26 11:52:21,338 - WDM - INFO - Get LATEST chromedriver version for google-chrome
2025-05-26 11:52:21,391 - WDM - INFO - Get LATEST chromedriver version for google-chrome
2025-05-26 11:52:21,877 - WDM - INFO - WebDriver version 136.0.7103.113 selected
2025-05-26 11:52:21,878 - WDM - INFO - Modern chrome version https://storage.googleapis.com/chrome-for-testing-public/136.0.7103.113/mac-arm64/chromedriver-mac-arm64.zip
2025-05-26 11:52:21,878 - WDM - INFO - About to download new driver from https://storage.googleapis.com/chrome-for-testing-public/136.0.7103.113/mac-arm64/chromedriver-mac-arm64.zip
2025-05-26 11:52:21,926 - WDM - INFO - Driver downloading response is 200
2025-05-26 11:52:23,861 - WDM - INFO - Get LATEST chromedriver version for google-chrome
2025-05-26 11:52:23,995 - WDM - INFO - Driver has been saved in cache [/Users/ruka/.wdm/drivers/chromedriver/mac64/136.0.7103.113]
2025-05-26 11:52:25,165 - group4py.src.scrape.selenium - INFO - Chrome WebDriver initialized successfully
2025-05-26 11:52:25,166 - group4py.src.scrape.selenium - INFO - Extracting all containers from https://unfccc.int/NDCREG
2025-05-26 11:52:25,166 - group4py.src.scrape.selenium - INFO - Loading page https://unfccc.int/NDCREG (attempt 1/3)
2025-05-26 11:52:26,566 - group4py.src.scrape.selenium - INFO - Added 6 cookies to driver
2025-05-26 11:52:26,977 - group4py.src.scrape.selenium - INFO - Page loaded successfully
2025-05-26 11:52:29,009 - group4py.src.scrape.selenium - INFO - Successfully extracted 217 containers
2025-05-26 11:52:29,010 - group4py.src.scrape.selenium - INFO - Extracting metadata from containers
2025-05-26 11:52:34,647 - group4py.src.scrape.selenium - INFO - Extracted metadata for 235 documents
2025-05-26 11:52:34,721 - group4py.src.scrape.selenium - INFO - WebDriver closed successfully
2025-05-26 11:52:34,721 - group4py.src.scrape.workflow - INFO - Scraped 235 documents from website
2025-05-26 11:52:34,721 - group4py.src.scrape.workflow - INFO - Step 3: Comparing documents to identify changes...
2025-05-26 11:52:34,721 - group4py.src.scrape.comparator - INFO - Comparing 206 existing docs with 235 new docs
2025-05-26 11:52:34,722 - group4py.src.scrape.comparator - INFO - Comparison results: 2 new, 0 updated, 0 removed
2025-05-26 11:52:34,722 - group4py.src.scrape.workflow - INFO - Step 4: Processing 2 new documents...
2025-05-26 11:52:34,722 - group4py.src.scrape.db_operations - INFO - Inserting 2 new documents into database
2025-05-26 11:52:34,722 - group4py.src.database - INFO - <DATABASE> Connecting...
2025-05-26 11:52:34,722 - group4py.src.database - INFO - <DATABASE> Getting engine...
2025-05-26 11:52:34,723 - group4py.src.database - INFO - <DATABASE> Engine created successfully
2025-05-26 11:52:35,588 - group4py.src.database - WARNING - <DATABASE> Connected to database successfully
2025-05-26 11:52:35,590 - group4py.src.database - INFO - <DATABASE> Uploading data into database...
2025-05-26 11:52:35,799 - group4py.src.database - INFO - <DATABASE> Uploaded to documents successfully
2025-05-26 11:52:35,800 - group4py.src.scrape.db_operations - INFO - Successfully inserted 2 new documents
2025-05-26 11:52:35,800 - group4py.src.scrape.workflow - INFO - Successfully inserted 2/2 new documents
2025-05-26 11:52:35,800 - group4py.src.scrape.workflow - INFO - Step 4.1: Downloading 2 new documents...
2025-05-26 11:52:35,801 - group4py.src.scrape.workflow - INFO - Downloading document: Cuba - Cuba NDC 3.0 from https://unfccc.int/sites/default/files/2025-02/REPUBLICA%20DE%20CUBA%20CND3.0.pdf
2025-05-26 11:52:35,802 - group4py.src.scrape.download - INFO - Using output directory: /Users/ruka/Desktop/rag-fact-sheets-4/data/pdfs
2025-05-26 11:52:35,802 - group4py.src.scrape.download - INFO - Downloading document from https://unfccc.int/sites/default/files/2025-02/REPUBLICA%20DE%20CUBA%20CND3.0.pdf
2025-05-26 11:52:36,011 - group4py.src.scrape.download - INFO - Document successfully downloaded to /Users/ruka/Desktop/rag-fact-sheets-4/data/pdfs/REPUBLICA DE CUBA CND3.0.pdf
2025-05-26 11:52:36,012 - group4py.src.scrape.workflow - INFO - Successfully downloaded document to /Users/ruka/Desktop/rag-fact-sheets-4/data/pdfs/REPUBLICA DE CUBA CND3.0.pdf
2025-05-26 11:52:36,012 - group4py.src.scrape.workflow - INFO - Downloading document: Papua New Guinea - Papua New Guinea Second NDC from https://unfccc.int/sites/default/files/NDC/2022-06/PNG%20Second%20NDC.pdf
2025-05-26 11:52:36,012 - group4py.src.scrape.download - INFO - Using output directory: /Users/ruka/Desktop/rag-fact-sheets-4/data/pdfs
2025-05-26 11:52:36,012 - group4py.src.scrape.download - INFO - Downloading document from https://unfccc.int/sites/default/files/NDC/2022-06/PNG%20Second%20NDC.pdf
2025-05-26 11:52:37,625 - group4py.src.scrape.download - INFO - Document successfully downloaded to /Users/ruka/Desktop/rag-fact-sheets-4/data/pdfs/PNG Second NDC.pdf
2025-05-26 11:52:37,626 - group4py.src.scrape.workflow - INFO - Successfully downloaded document to /Users/ruka/Desktop/rag-fact-sheets-4/data/pdfs/PNG Second NDC.pdf
2025-05-26 11:52:37,626 - group4py.src.scrape.workflow - INFO - Successfully downloaded 2/2 new documents
2025-05-26 11:52:37,626 - group4py.src.scrape.workflow - INFO - Step 5: No documents to update
2025-05-26 11:52:37,626 - group4py.src.scrape.workflow - INFO - Step 6: No removed documents found
2025-05-26 11:52:37,626 - group4py.src.scrape.workflow - INFO - ============================================================
2025-05-26 11:52:37,626 - group4py.src.scrape.workflow - INFO - WORKFLOW SUMMARY:
2025-05-26 11:52:37,626 - group4py.src.scrape.workflow - INFO -   Documents in database: 206
2025-05-26 11:52:37,626 - group4py.src.scrape.workflow - INFO -   Documents on website: 235
2025-05-26 11:52:37,627 - group4py.src.scrape.workflow - INFO -   New documents inserted: 2
2025-05-26 11:52:37,627 - group4py.src.scrape.workflow - INFO -   New documents downloaded: 2
2025-05-26 11:52:37,627 - group4py.src.scrape.workflow - INFO -   Documents updated: 0
2025-05-26 11:52:37,627 - group4py.src.scrape.workflow - INFO -   Documents removed (not processed): 0
2025-05-26 11:52:37,627 - group4py.src.scrape.workflow - INFO - ============================================================
2025-05-26 11:52:37,627 - group4py.src.scrape.workflow - INFO - NDC document scraping workflow completed successfully!
2025-05-26 12:01:07,205 - group4py.src.scrape.workflow - INFO - Starting NDC document scraping workflow
2025-05-26 12:01:07,205 - group4py.src.scrape.workflow - INFO - Step 1: Retrieving existing documents from database...
2025-05-26 12:01:07,205 - group4py.src.scrape.db_operations - INFO - Retrieving existing documents from database
2025-05-26 12:01:07,206 - group4py.src.database - INFO - <DATABASE> Connecting...
2025-05-26 12:01:07,206 - group4py.src.database - INFO - <DATABASE> Getting engine...
2025-05-26 12:01:07,220 - group4py.src.database - INFO - <DATABASE> Engine created successfully
2025-05-26 12:01:11,056 - group4py.src.database - WARNING - <DATABASE> Connected to database successfully
2025-05-26 12:01:11,365 - group4py.src.scrape.db_operations - INFO - Retrieved 208 existing documents from database
2025-05-26 12:01:11,376 - group4py.src.scrape.workflow - INFO - Found 208 existing documents in database
2025-05-26 12:01:11,376 - group4py.src.scrape.workflow - INFO - Step 2: Scraping fresh documents from website...
2025-05-26 12:01:11,377 - WDM - INFO - ====== WebDriver manager ======
2025-05-26 12:01:11,594 - WDM - INFO - Get LATEST chromedriver version for google-chrome
2025-05-26 12:01:11,656 - WDM - INFO - Get LATEST chromedriver version for google-chrome
2025-05-26 12:01:12,012 - WDM - INFO - Driver [/Users/ruka/.wdm/drivers/chromedriver/mac64/136.0.7103.113/chromedriver-mac-arm64/chromedriver] found in cache
2025-05-26 12:01:13,098 - group4py.src.scrape.selenium - INFO - Chrome WebDriver initialized successfully
2025-05-26 12:01:13,100 - group4py.src.scrape.selenium - INFO - Extracting all containers from https://unfccc.int/NDCREG
2025-05-26 12:01:13,100 - group4py.src.scrape.selenium - INFO - Loading page https://unfccc.int/NDCREG (attempt 1/3)
2025-05-26 12:01:14,555 - group4py.src.scrape.selenium - INFO - Added 6 cookies to driver
2025-05-26 12:01:15,061 - group4py.src.scrape.selenium - INFO - Page loaded successfully
2025-05-26 12:01:17,094 - group4py.src.scrape.selenium - INFO - Successfully extracted 217 containers
2025-05-26 12:01:17,094 - group4py.src.scrape.selenium - INFO - Extracting metadata from containers
2025-05-26 12:01:22,344 - group4py.src.scrape.selenium - INFO - Extracted metadata for 235 documents
2025-05-26 12:01:22,424 - group4py.src.scrape.selenium - INFO - WebDriver closed successfully
2025-05-26 12:01:22,424 - group4py.src.scrape.workflow - INFO - Scraped 235 documents from website
2025-05-26 12:01:22,424 - group4py.src.scrape.workflow - INFO - Step 3: Comparing documents to identify changes...
2025-05-26 12:01:22,424 - group4py.src.scrape.comparator - INFO - Comparing 208 existing docs with 235 new docs
2025-05-26 12:01:22,425 - group4py.src.scrape.comparator - INFO - Comparison results: 0 new, 0 updated, 0 removed
2025-05-26 12:01:22,425 - group4py.src.scrape.workflow - INFO - Step 4: No new documents to process
2025-05-26 12:01:22,425 - group4py.src.scrape.workflow - INFO - Step 5: No documents to update
2025-05-26 12:01:22,425 - group4py.src.scrape.workflow - INFO - Step 6: No removed documents found
2025-05-26 12:01:22,425 - group4py.src.scrape.workflow - INFO - ============================================================
2025-05-26 12:01:22,425 - group4py.src.scrape.workflow - INFO - WORKFLOW SUMMARY:
2025-05-26 12:01:22,425 - group4py.src.scrape.workflow - INFO -   Documents in database: 208
2025-05-26 12:01:22,425 - group4py.src.scrape.workflow - INFO -   Documents on website: 235
2025-05-26 12:01:22,425 - group4py.src.scrape.workflow - INFO -   New documents inserted: 0
2025-05-26 12:01:22,425 - group4py.src.scrape.workflow - INFO -   New documents downloaded: 0
2025-05-26 12:01:22,425 - group4py.src.scrape.workflow - INFO -   Documents updated: 0
2025-05-26 12:01:22,425 - group4py.src.scrape.workflow - INFO -   Documents removed (not processed): 0
2025-05-26 12:01:22,425 - group4py.src.scrape.workflow - INFO - ============================================================
2025-05-26 12:01:22,425 - group4py.src.scrape.workflow - INFO - NDC document scraping workflow completed successfully!
2025-05-27 14:12:12,494 - group4py.src.scrape.workflow - INFO - Starting NDC document scraping workflow
2025-05-27 14:12:12,494 - group4py.src.scrape.workflow - INFO - Step 1: Retrieving existing documents from database...
2025-05-27 14:12:12,494 - group4py.src.scrape.db_operations - INFO - Retrieving existing documents from database
2025-05-27 14:12:12,494 - group4py.src.scrape.db_operations - ERROR - Database connection error: Connection.__init__() missing 1 required positional argument: 'config'
2025-05-27 14:12:12,494 - group4py.src.scrape.workflow - ERROR - Unexpected error during scraping workflow: Database connection failed: Connection.__init__() missing 1 required positional argument: 'config'
2025-05-27 14:12:12,494 - group4py.src.scrape.workflow - ERROR - Traceback: Traceback (most recent call last):
  File "/Users/ruka/Desktop/rag-fact-sheets-4/group4py/src/scrape/db_operations.py", line 28, in retrieve_existing_documents
    db = Connection()
TypeError: Connection.__init__() missing 1 required positional argument: 'config'

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/Users/ruka/Desktop/rag-fact-sheets-4/group4py/src/scrape/workflow.py", line 47, in run_scraping_workflow
    existing_docs = retrieve_existing_documents()
  File "/Users/ruka/Desktop/rag-fact-sheets-4/group4py/src/scrape/db_operations.py", line 47, in retrieve_existing_documents
    raise DatabaseConnectionError(f"Database connection failed: {str(e)}") from e
group4py.src.scrape.exceptions.DatabaseConnectionError: Database connection failed: Connection.__init__() missing 1 required positional argument: 'config'

2025-05-27 14:16:02,967 - group4py.src.scrape.workflow - INFO - Starting NDC document scraping workflow
2025-05-27 14:16:02,967 - group4py.src.scrape.workflow - INFO - Step 1: Retrieving existing documents from database...
2025-05-27 14:16:02,967 - group4py.src.scrape.db_operations - INFO - Retrieving existing documents from database
2025-05-27 14:16:03,955 - group4py.src.database - INFO - Successfully connected to database
2025-05-27 14:16:04,081 - group4py.src.scrape.db_operations - ERROR - Error querying database: (psycopg2.errors.UndefinedColumn) column doc_chunks.content_hash does not exist
LINE 1: ...d2vec_embedding AS doc_chunks_word2vec_embedding, doc_chunks...
                                                             ^

[SQL: SELECT doc_chunks.id AS doc_chunks_id, doc_chunks.doc_id AS doc_chunks_doc_id, doc_chunks.content AS doc_chunks_content, doc_chunks.chunk_index AS doc_chunks_chunk_index, doc_chunks.paragraph AS doc_chunks_paragraph, doc_chunks.language AS doc_chunks_language, doc_chunks.transformer_embedding AS doc_chunks_transformer_embedding, doc_chunks.word2vec_embedding AS doc_chunks_word2vec_embedding, doc_chunks.content_hash AS doc_chunks_content_hash, doc_chunks.chunk_data AS doc_chunks_chunk_data, doc_chunks.created_at AS doc_chunks_created_at, doc_chunks.updated_at AS doc_chunks_updated_at 
FROM doc_chunks 
WHERE %(param_1)s::UUID = doc_chunks.doc_id]
[parameters: {'param_1': UUID('bd8dd183-54b5-5e82-a2bf-59f476c119d6')}]
(Background on this error at: https://sqlalche.me/e/20/f405)
2025-05-27 14:16:04,091 - group4py.src.scrape.workflow - ERROR - Unexpected error during scraping workflow: Database query failed: (psycopg2.errors.UndefinedColumn) column doc_chunks.content_hash does not exist
LINE 1: ...d2vec_embedding AS doc_chunks_word2vec_embedding, doc_chunks...
                                                             ^

[SQL: SELECT doc_chunks.id AS doc_chunks_id, doc_chunks.doc_id AS doc_chunks_doc_id, doc_chunks.content AS doc_chunks_content, doc_chunks.chunk_index AS doc_chunks_chunk_index, doc_chunks.paragraph AS doc_chunks_paragraph, doc_chunks.language AS doc_chunks_language, doc_chunks.transformer_embedding AS doc_chunks_transformer_embedding, doc_chunks.word2vec_embedding AS doc_chunks_word2vec_embedding, doc_chunks.content_hash AS doc_chunks_content_hash, doc_chunks.chunk_data AS doc_chunks_chunk_data, doc_chunks.created_at AS doc_chunks_created_at, doc_chunks.updated_at AS doc_chunks_updated_at 
FROM doc_chunks 
WHERE %(param_1)s::UUID = doc_chunks.doc_id]
[parameters: {'param_1': UUID('bd8dd183-54b5-5e82-a2bf-59f476c119d6')}]
(Background on this error at: https://sqlalche.me/e/20/f405)
2025-05-27 14:16:04,100 - group4py.src.scrape.workflow - ERROR - Traceback: Traceback (most recent call last):
  File "/Users/ruka/Desktop/rag-fact-sheets-4/group4venv/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1963, in _exec_single_context
    self.dialect.do_execute(
  File "/Users/ruka/Desktop/rag-fact-sheets-4/group4venv/lib/python3.10/site-packages/sqlalchemy/engine/default.py", line 943, in do_execute
    cursor.execute(statement, parameters)
psycopg2.errors.UndefinedColumn: column doc_chunks.content_hash does not exist
LINE 1: ...d2vec_embedding AS doc_chunks_word2vec_embedding, doc_chunks...
                                                             ^


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/Users/ruka/Desktop/rag-fact-sheets-4/group4py/src/scrape/db_operations.py", line 36, in retrieve_existing_documents
    existing_docs = [_convert_db_to_model(db_doc) for db_doc in db_documents]
  File "/Users/ruka/Desktop/rag-fact-sheets-4/group4py/src/scrape/db_operations.py", line 36, in <listcomp>
    existing_docs = [_convert_db_to_model(db_doc) for db_doc in db_documents]
  File "/Users/ruka/Desktop/rag-fact-sheets-4/group4py/src/scrape/db_operations.py", line 179, in _convert_db_to_model
    chunks=db_doc.chunks,
  File "/Users/ruka/Desktop/rag-fact-sheets-4/group4venv/lib/python3.10/site-packages/sqlalchemy/orm/attributes.py", line 566, in __get__
    return self.impl.get(state, dict_)  # type: ignore[no-any-return]
  File "/Users/ruka/Desktop/rag-fact-sheets-4/group4venv/lib/python3.10/site-packages/sqlalchemy/orm/attributes.py", line 1086, in get
    value = self._fire_loader_callables(state, key, passive)
  File "/Users/ruka/Desktop/rag-fact-sheets-4/group4venv/lib/python3.10/site-packages/sqlalchemy/orm/attributes.py", line 1121, in _fire_loader_callables
    return self.callable_(state, passive)
  File "/Users/ruka/Desktop/rag-fact-sheets-4/group4venv/lib/python3.10/site-packages/sqlalchemy/orm/strategies.py", line 978, in _load_for_state
    return self._emit_lazyload(
  File "/Users/ruka/Desktop/rag-fact-sheets-4/group4venv/lib/python3.10/site-packages/sqlalchemy/orm/strategies.py", line 1141, in _emit_lazyload
    result = session.execute(
  File "/Users/ruka/Desktop/rag-fact-sheets-4/group4venv/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 2365, in execute
    return self._execute_internal(
  File "/Users/ruka/Desktop/rag-fact-sheets-4/group4venv/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 2251, in _execute_internal
    result: Result[Any] = compile_state_cls.orm_execute_statement(
  File "/Users/ruka/Desktop/rag-fact-sheets-4/group4venv/lib/python3.10/site-packages/sqlalchemy/orm/context.py", line 306, in orm_execute_statement
    result = conn.execute(
  File "/Users/ruka/Desktop/rag-fact-sheets-4/group4venv/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1415, in execute
    return meth(
  File "/Users/ruka/Desktop/rag-fact-sheets-4/group4venv/lib/python3.10/site-packages/sqlalchemy/sql/elements.py", line 523, in _execute_on_connection
    return connection._execute_clauseelement(
  File "/Users/ruka/Desktop/rag-fact-sheets-4/group4venv/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1637, in _execute_clauseelement
    ret = self._execute_context(
  File "/Users/ruka/Desktop/rag-fact-sheets-4/group4venv/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1842, in _execute_context
    return self._exec_single_context(
  File "/Users/ruka/Desktop/rag-fact-sheets-4/group4venv/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1982, in _exec_single_context
    self._handle_dbapi_exception(
  File "/Users/ruka/Desktop/rag-fact-sheets-4/group4venv/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 2351, in _handle_dbapi_exception
    raise sqlalchemy_exception.with_traceback(exc_info[2]) from e
  File "/Users/ruka/Desktop/rag-fact-sheets-4/group4venv/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1963, in _exec_single_context
    self.dialect.do_execute(
  File "/Users/ruka/Desktop/rag-fact-sheets-4/group4venv/lib/python3.10/site-packages/sqlalchemy/engine/default.py", line 943, in do_execute
    cursor.execute(statement, parameters)
sqlalchemy.exc.ProgrammingError: (psycopg2.errors.UndefinedColumn) column doc_chunks.content_hash does not exist
LINE 1: ...d2vec_embedding AS doc_chunks_word2vec_embedding, doc_chunks...
                                                             ^

[SQL: SELECT doc_chunks.id AS doc_chunks_id, doc_chunks.doc_id AS doc_chunks_doc_id, doc_chunks.content AS doc_chunks_content, doc_chunks.chunk_index AS doc_chunks_chunk_index, doc_chunks.paragraph AS doc_chunks_paragraph, doc_chunks.language AS doc_chunks_language, doc_chunks.transformer_embedding AS doc_chunks_transformer_embedding, doc_chunks.word2vec_embedding AS doc_chunks_word2vec_embedding, doc_chunks.content_hash AS doc_chunks_content_hash, doc_chunks.chunk_data AS doc_chunks_chunk_data, doc_chunks.created_at AS doc_chunks_created_at, doc_chunks.updated_at AS doc_chunks_updated_at 
FROM doc_chunks 
WHERE %(param_1)s::UUID = doc_chunks.doc_id]
[parameters: {'param_1': UUID('bd8dd183-54b5-5e82-a2bf-59f476c119d6')}]
(Background on this error at: https://sqlalche.me/e/20/f405)

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/Users/ruka/Desktop/rag-fact-sheets-4/group4py/src/scrape/workflow.py", line 47, in run_scraping_workflow
    existing_docs = retrieve_existing_documents()
  File "/Users/ruka/Desktop/rag-fact-sheets-4/group4py/src/scrape/db_operations.py", line 41, in retrieve_existing_documents
    raise DatabaseConnectionError(f"Database query failed: {str(e)}") from e
group4py.src.scrape.exceptions.DatabaseConnectionError: Database query failed: (psycopg2.errors.UndefinedColumn) column doc_chunks.content_hash does not exist
LINE 1: ...d2vec_embedding AS doc_chunks_word2vec_embedding, doc_chunks...
                                                             ^

[SQL: SELECT doc_chunks.id AS doc_chunks_id, doc_chunks.doc_id AS doc_chunks_doc_id, doc_chunks.content AS doc_chunks_content, doc_chunks.chunk_index AS doc_chunks_chunk_index, doc_chunks.paragraph AS doc_chunks_paragraph, doc_chunks.language AS doc_chunks_language, doc_chunks.transformer_embedding AS doc_chunks_transformer_embedding, doc_chunks.word2vec_embedding AS doc_chunks_word2vec_embedding, doc_chunks.content_hash AS doc_chunks_content_hash, doc_chunks.chunk_data AS doc_chunks_chunk_data, doc_chunks.created_at AS doc_chunks_created_at, doc_chunks.updated_at AS doc_chunks_updated_at 
FROM doc_chunks 
WHERE %(param_1)s::UUID = doc_chunks.doc_id]
[parameters: {'param_1': UUID('bd8dd183-54b5-5e82-a2bf-59f476c119d6')}]
(Background on this error at: https://sqlalche.me/e/20/f405)

2025-05-27 16:49:26,501 - group4py.src.scrape.workflow - INFO - Starting NDC document scraping workflow
2025-05-27 16:49:26,501 - group4py.src.scrape.workflow - INFO - Step 1: Retrieving existing documents from database...
2025-05-27 16:49:26,501 - scrape.db_operations - INFO - Retrieving existing documents from database
2025-05-27 16:49:27,771 - databases.auth - ERROR - Database session error: 'NDCDocumentORM' object has no attribute 'chunks'
2025-05-27 16:49:27,772 - scrape.db_operations - ERROR - Database connection error: 'NDCDocumentORM' object has no attribute 'chunks'
2025-05-27 16:49:27,772 - group4py.src.scrape.workflow - ERROR - Unexpected error during scraping workflow: Database connection failed: 'NDCDocumentORM' object has no attribute 'chunks'
2025-05-27 16:49:27,773 - group4py.src.scrape.workflow - ERROR - Traceback: Traceback (most recent call last):
  File "/Users/ruka/Desktop/rag-fact-sheets-4/group4py/src/scrape/db_operations.py", line 39, in retrieve_existing_documents
    existing_docs = [_convert_db_to_model(db_doc) for db_doc in db_documents]
  File "/Users/ruka/Desktop/rag-fact-sheets-4/group4py/src/scrape/db_operations.py", line 39, in <listcomp>
    existing_docs = [_convert_db_to_model(db_doc) for db_doc in db_documents]
  File "/Users/ruka/Desktop/rag-fact-sheets-4/group4py/src/scrape/db_operations.py", line 160, in _convert_db_to_model
    chunks=db_doc.chunks,
AttributeError: 'NDCDocumentORM' object has no attribute 'chunks'

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/Users/ruka/Desktop/rag-fact-sheets-4/group4py/src/scrape/workflow.py", line 52, in run_scraping_workflow
    existing_docs = retrieve_existing_documents()
  File "/Users/ruka/Desktop/rag-fact-sheets-4/group4py/src/scrape/db_operations.py", line 44, in retrieve_existing_documents
    raise DatabaseConnectionError(f"Database connection failed: {str(e)}") from e
scrape.exceptions.DatabaseConnectionError: Database connection failed: 'NDCDocumentORM' object has no attribute 'chunks'

2025-05-27 16:50:14,497 - group4py.src.scrape.workflow - INFO - Starting NDC document scraping workflow
2025-05-27 16:50:14,497 - group4py.src.scrape.workflow - INFO - Step 1: Retrieving existing documents from database...
2025-05-27 16:50:14,497 - scrape.db_operations - INFO - Retrieving existing documents from database
2025-05-27 16:50:15,179 - scrape.db_operations - INFO - Retrieved 207 existing documents from database
2025-05-27 16:50:15,221 - group4py.src.scrape.workflow - INFO - Found 207 existing documents in database
2025-05-27 16:50:15,221 - group4py.src.scrape.workflow - INFO - Step 2: Scraping fresh documents from website...
2025-05-27 16:50:15,221 - WDM - INFO - ====== WebDriver manager ======
2025-05-27 16:50:15,762 - WDM - INFO - Get LATEST chromedriver version for google-chrome
2025-05-27 16:50:16,023 - WDM - INFO - Get LATEST chromedriver version for google-chrome
2025-05-27 16:50:16,071 - WDM - INFO - Get LATEST chromedriver version for google-chrome
2025-05-27 16:50:16,224 - WDM - INFO - WebDriver version 136.0.7103.113 selected
2025-05-27 16:50:16,226 - WDM - INFO - Modern chrome version https://storage.googleapis.com/chrome-for-testing-public/136.0.7103.113/mac-arm64/chromedriver-mac-arm64.zip
2025-05-27 16:50:16,226 - WDM - INFO - About to download new driver from https://storage.googleapis.com/chrome-for-testing-public/136.0.7103.113/mac-arm64/chromedriver-mac-arm64.zip
2025-05-27 16:50:16,287 - WDM - INFO - Driver downloading response is 200
2025-05-27 16:50:19,264 - WDM - INFO - Get LATEST chromedriver version for google-chrome
2025-05-27 16:50:19,426 - WDM - INFO - Driver has been saved in cache [/Users/ruka/.wdm/drivers/chromedriver/mac64/136.0.7103.113]
2025-05-27 16:50:20,633 - scrape.selenium - INFO - Chrome WebDriver initialized successfully
2025-05-27 16:50:20,634 - scrape.selenium - INFO - Extracting all containers from https://unfccc.int/NDCREG
2025-05-27 16:50:20,634 - scrape.selenium - INFO - Loading page https://unfccc.int/NDCREG (attempt 1/3)
2025-05-27 16:50:22,105 - scrape.selenium - INFO - Added 6 cookies to driver
2025-05-27 16:50:22,518 - scrape.selenium - INFO - Page loaded successfully
2025-05-27 16:50:24,550 - scrape.selenium - INFO - Successfully extracted 217 containers
2025-05-27 16:50:24,551 - scrape.selenium - INFO - Extracting metadata from containers
2025-05-27 16:50:29,871 - scrape.selenium - INFO - Extracted metadata for 235 documents
2025-05-27 16:50:29,949 - scrape.selenium - INFO - WebDriver closed successfully
2025-05-27 16:50:29,950 - group4py.src.scrape.workflow - INFO - Scraped 235 documents from website
2025-05-27 16:50:29,950 - group4py.src.scrape.workflow - INFO - Step 3: Comparing documents to identify changes...
2025-05-27 16:50:29,950 - scrape.comparator - INFO - Comparing 207 existing docs with 235 new docs
2025-05-27 16:50:29,950 - scrape.comparator - INFO - Comparison results: 1 new, 0 updated, 0 removed
2025-05-27 16:50:29,950 - group4py.src.scrape.workflow - INFO - Step 4: Processing 1 new documents...
2025-05-27 16:50:29,950 - scrape.db_operations - INFO - Inserting 1 new documents into database
2025-05-27 16:50:30,960 - scrape.db_operations - INFO - Successfully inserted 1 new documents
2025-05-27 16:50:30,961 - group4py.src.scrape.workflow - INFO - Successfully inserted 1/1 new documents
2025-05-27 16:50:30,961 - group4py.src.scrape.workflow - INFO - Step 4.1: Downloading 1 new documents...
2025-05-27 16:50:30,961 - group4py.src.scrape.workflow - INFO - Downloading document: Liberia - Liberia First NDC (Updated submission) from https://unfccc.int/sites/default/files/NDC/2022-06/Liberia%27s%20Updated%20NDC_RL_FINAL%20%28002%29.pdf
2025-05-27 16:50:30,962 - scrape.download - INFO - Using output directory: /Users/ruka/Desktop/rag-fact-sheets-4/data/pdfs
2025-05-27 16:50:30,962 - scrape.download - INFO - Downloading document from https://unfccc.int/sites/default/files/NDC/2022-06/Liberia%27s%20Updated%20NDC_RL_FINAL%20%28002%29.pdf
2025-05-27 16:50:31,583 - scrape.download - INFO - Document successfully downloaded to /Users/ruka/Desktop/rag-fact-sheets-4/data/pdfs/Liberia's Updated NDC_RL_FINAL (002).pdf
2025-05-27 16:50:31,585 - group4py.src.scrape.workflow - INFO - Successfully downloaded document to /Users/ruka/Desktop/rag-fact-sheets-4/data/pdfs/Liberia's Updated NDC_RL_FINAL (002).pdf
2025-05-27 16:50:31,585 - group4py.src.scrape.workflow - INFO - Successfully downloaded 1/1 new documents
2025-05-27 16:50:31,585 - group4py.src.scrape.workflow - INFO - Step 5: No documents to update
2025-05-27 16:50:31,585 - group4py.src.scrape.workflow - INFO - Step 6: No removed documents found
2025-05-27 16:50:31,585 - group4py.src.scrape.workflow - INFO - ============================================================
2025-05-27 16:50:31,585 - group4py.src.scrape.workflow - INFO - WORKFLOW SUMMARY:
2025-05-27 16:50:31,586 - group4py.src.scrape.workflow - INFO -   Documents in database: 207
2025-05-27 16:50:31,586 - group4py.src.scrape.workflow - INFO -   Documents on website: 235
2025-05-27 16:50:31,586 - group4py.src.scrape.workflow - INFO -   New documents inserted: 1
2025-05-27 16:50:31,586 - group4py.src.scrape.workflow - INFO -   New documents downloaded: 1
2025-05-27 16:50:31,587 - group4py.src.scrape.workflow - INFO -   Documents updated: 0
2025-05-27 16:50:31,587 - group4py.src.scrape.workflow - INFO -   Documents removed (not processed): 0
2025-05-27 16:50:31,587 - group4py.src.scrape.workflow - INFO - ============================================================
2025-05-27 16:50:31,587 - group4py.src.scrape.workflow - INFO - NDC document scraping workflow completed successfully!
2025-05-27 17:05:39,701 - group4py.src.scrape.workflow - INFO - Starting NDC document scraping workflow
2025-05-27 17:05:39,702 - group4py.src.scrape.workflow - INFO - Step 1: Retrieving existing documents from database...
2025-05-27 17:05:39,702 - scrape.db_operations - INFO - Retrieving existing documents from database
2025-05-27 17:05:39,702 - databases.auth - ERROR - Database session error: '_GeneratorContextManager' object has no attribute 'query'
2025-05-27 17:05:39,702 - scrape.db_operations - ERROR - Database connection error: '_GeneratorContextManager' object has no attribute 'query'
2025-05-27 17:05:39,702 - group4py.src.scrape.workflow - ERROR - Unexpected error during scraping workflow: Database connection failed: '_GeneratorContextManager' object has no attribute 'query'
2025-05-27 17:05:39,702 - group4py.src.scrape.workflow - ERROR - Traceback: Traceback (most recent call last):
  File "/Users/ruka/Desktop/rag-fact-sheets-4/group4py/src/scrape/db_operations.py", line 37, in retrieve_existing_documents
    db_documents = session.query(NDCDocumentORM).all()
AttributeError: '_GeneratorContextManager' object has no attribute 'query'

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/Users/ruka/Desktop/rag-fact-sheets-4/group4py/src/scrape/workflow.py", line 52, in run_scraping_workflow
    existing_docs = retrieve_existing_documents()
  File "/Users/ruka/Desktop/rag-fact-sheets-4/group4py/src/scrape/db_operations.py", line 43, in retrieve_existing_documents
    raise DatabaseConnectionError(f"Database connection failed: {str(e)}") from e
scrape.exceptions.DatabaseConnectionError: Database connection failed: '_GeneratorContextManager' object has no attribute 'query'

2025-05-27 17:08:26,773 - group4py.src.scrape.workflow - INFO - Starting NDC document scraping workflow
2025-05-27 17:08:26,773 - group4py.src.scrape.workflow - INFO - Step 1: Retrieving existing documents from database...
2025-05-27 17:08:26,773 - scrape.db_operations - INFO - Retrieving existing documents from database
2025-05-27 17:08:27,366 - scrape.db_operations - INFO - Retrieved 208 existing documents from database
2025-05-27 17:08:27,386 - group4py.src.scrape.workflow - INFO - Found 208 existing documents in database
2025-05-27 17:08:27,386 - group4py.src.scrape.workflow - INFO - Step 2: Scraping fresh documents from website...
2025-05-27 17:08:27,386 - WDM - INFO - ====== WebDriver manager ======
2025-05-27 17:08:27,590 - WDM - INFO - Get LATEST chromedriver version for google-chrome
2025-05-27 17:08:27,805 - WDM - INFO - Get LATEST chromedriver version for google-chrome
2025-05-27 17:08:27,845 - WDM - INFO - Driver [/Users/ruka/.wdm/drivers/chromedriver/mac64/136.0.7103.113/chromedriver-mac-arm64/chromedriver] found in cache
2025-05-27 17:08:28,977 - scrape.selenium - INFO - Chrome WebDriver initialized successfully
2025-05-27 17:08:28,977 - scrape.selenium - INFO - Extracting all containers from https://unfccc.int/NDCREG
2025-05-27 17:08:28,977 - scrape.selenium - INFO - Loading page https://unfccc.int/NDCREG (attempt 1/3)
2025-05-27 17:08:30,149 - scrape.selenium - INFO - Added 6 cookies to driver
2025-05-27 17:08:30,606 - scrape.selenium - INFO - Page loaded successfully
2025-05-27 17:08:32,627 - scrape.selenium - INFO - Successfully extracted 217 containers
2025-05-27 17:08:32,627 - scrape.selenium - INFO - Extracting metadata from containers
2025-05-27 17:08:33,373 - urllib3.connectionpool - WARNING - Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x10cb622c0>: Failed to establish a new connection: [Errno 61] Connection refused')': /session/6c2f0540f02eda9525e0b76c808d0e33
2025-05-27 17:08:33,374 - urllib3.connectionpool - WARNING - Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x10cb629e0>: Failed to establish a new connection: [Errno 61] Connection refused')': /session/6c2f0540f02eda9525e0b76c808d0e33
2025-05-27 17:08:33,374 - urllib3.connectionpool - WARNING - Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x10cb62b30>: Failed to establish a new connection: [Errno 61] Connection refused')': /session/6c2f0540f02eda9525e0b76c808d0e33
2025-05-27 17:08:33,375 - scrape.selenium - INFO - WebDriver closed successfully
